{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch6 Learning to Classify Text\n",
    "\n",
    "本章的目標是回答幾個問題:\n",
    "\n",
    "1. 如何辨識能用來分類的顯著特徵?\n",
    "2. 如何建立language model來自動完成文字處理作業?\n",
    "3. 從這些model中可以學到什麼?\n",
    "\n",
    "\n",
    "## Supervised Classification\n",
    "\n",
    "**Classification**將輸入標上正確的**class label**。**Supervised**是指預先得到正確標記的training data。\n",
    "\n",
    "例如我們想根據姓氏的結尾判斷是男或女："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Dannye', 'F'),\n",
       " (u'Alston', 'M'),\n",
       " (u'Conroy', 'M'),\n",
       " (u'Bret', 'M'),\n",
       " (u'Estele', 'F'),\n",
       " (u'Michaelina', 'F'),\n",
       " (u'Lurleen', 'F'),\n",
       " (u'Chevy', 'M'),\n",
       " (u'Carolynn', 'F'),\n",
       " (u'Blaine', 'M')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = [(n,'M') for n in names.words('male.txt')] + [(n,'F') for n in names.words('female.txt')]\n",
    "random.shuffle(name)\n",
    "name[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備一個function，用來產生feature，這裡的feature是用最後的英文字母。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_feature(name): return {'last_letter': name[-1]}\n",
    "featuresets = [(gender_feature(n), g) for (n,g) in name]\n",
    "train, test = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify({'last_letter': 'a'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = u'k'                M : F      =     44.9 : 1.0\n",
      "             last_letter = u'a'                F : M      =     35.8 : 1.0\n",
      "             last_letter = u'f'                M : F      =     15.2 : 1.0\n",
      "             last_letter = u'p'                M : F      =     11.1 : 1.0\n",
      "             last_letter = u'v'                M : F      =     10.5 : 1.0\n",
      "             last_letter = u'd'                M : F      =     10.0 : 1.0\n",
      "             last_letter = u'm'                M : F      =      9.1 : 1.0\n",
      "             last_letter = u'o'                M : F      =      8.4 : 1.0\n",
      "             last_letter = u'r'                M : F      =      6.8 : 1.0\n",
      "             last_letter = u'g'                M : F      =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嘗試不同的feature，例如加入第一個字母，或加入姓名的長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_feature(name): return {'last_letter': name[-1], 'first_letter': name[0]}\n",
    "featuresets = [(gender_feature(n), g) for (n,g) in name]\n",
    "train, test = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_feature(name): return {'last_letter': name[-1], 'first_letter': name[0], 'len': len(name)}\n",
    "featuresets = [(gender_feature(n), g) for (n,g) in name]\n",
    "train, test = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.762"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = u'k'                M : F      =     44.9 : 1.0\n",
      "             last_letter = u'a'                F : M      =     35.8 : 1.0\n",
      "             last_letter = u'f'                M : F      =     15.2 : 1.0\n",
      "             last_letter = u'p'                M : F      =     11.1 : 1.0\n",
      "             last_letter = u'v'                M : F      =     10.5 : 1.0\n",
      "             last_letter = u'd'                M : F      =     10.0 : 1.0\n",
      "             last_letter = u'm'                M : F      =      9.1 : 1.0\n",
      "             last_letter = u'o'                M : F      =      8.4 : 1.0\n",
      "             last_letter = u'r'                M : F      =      6.8 : 1.0\n",
      "             last_letter = u'g'                M : F      =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification\n",
    "\n",
    "`movie_reviews`裡面已經分好正面和負面兩種評語，我們的目標是根據字彙，預測評語是正面或負面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'neg', u'pos']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([u'neg/cv000_29416.txt',\n",
       "  u'neg/cv001_19502.txt',\n",
       "  u'neg/cv002_17424.txt',\n",
       "  u'neg/cv003_12683.txt',\n",
       "  u'neg/cv004_12641.txt'],\n",
       " [u'pos/cv000_29590.txt',\n",
       "  u'pos/cv001_18431.txt',\n",
       "  u'pos/cv002_15918.txt',\n",
       "  u'pos/cv003_11664.txt',\n",
       "  u'pos/cv004_11636.txt'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids('neg')[:5], movie_reviews.fileids('pos')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(f)), c)\n",
    "             for c in movie_reviews.categories() for f in movie_reviews.fileids(c)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u',', u'the', u'.', u'a', u'and']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選擇2000個字作為feature\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = [w for (w, c) in all_words.most_common()[:2000]]\n",
    "word_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features(doc):\n",
    "    document_words = set(doc)  # 自動排除重複字\n",
    "    features = {w: w in document_words for w in word_features}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the', u'she']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = document_features(['the', 'she'])\n",
    "[key for key in tmp if tmp[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "train, test = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  seagal = True              neg : pos    =     12.3 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.2 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.4 : 1.0\n",
      "             wonderfully = True              pos : neg    =      6.9 : 1.0\n",
      "                   damon = True              pos : neg    =      5.9 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.7 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.7 : 1.0\n",
      "                   waste = True              neg : pos    =      5.4 : 1.0\n",
      "                    jedi = True              pos : neg    =      5.3 : 1.0\n",
      "                   awful = True              neg : pos    =      5.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging\n",
    "\n",
    "在Chapter 5有介紹過regular expression tagger，當時是用手動設定規則，這裡嘗試用classifier找出規則。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "fdist = nltk.FreqDist()\n",
    "fdist.update([w[-1:] for w in brown.words()])\n",
    "fdist.update([w[-2:] for w in brown.words()])\n",
    "fdist.update([w[-3:] for w in brown.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'e', u',', u'.', u's', u'd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_suf = [k for (k, c) in fdist.most_common()[:100]]\n",
    "common_suf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {suffix:word.lower().endswith(suffix) for suffix in common_suf}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10055"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(pos_features(n), pos) for (n, pos) in brown.tagged_words(categories='news')]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train, test = featuresets[size:], featuresets[:size]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6248632521133765"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train)\n",
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the=False? ............................................ .\n",
      "  ,=False? ............................................ .\n",
      "    s=False? .......................................... .\n",
      "      .=False? ........................................ .\n",
      "        of=False? ..................................... .\n",
      "          and=False? .................................. .\n",
      "            a=False? .................................. .\n",
      "              in=False? ............................... .\n",
      "                ed=False? ............................. .\n",
      "                  to=False? ........................... .\n",
      "                  to=True? ............................ TO\n",
      "                ed=True? .............................. VBN\n",
      "              in=True? ................................ IN\n",
      "            a=True? ................................... AT\n",
      "          and=True? ................................... CC\n",
      "        of=True? ...................................... IN\n",
      "      .=True? ......................................... .\n",
      "    s=True? ........................................... PP$\n",
      "      is=False? ....................................... PP$\n",
      "        was=False? .................................... PP$\n",
      "          as=False? ................................... PP$\n",
      "            's=False? ................................. PP$\n",
      "              ss=False? ............................... NNS\n",
      "              ss=True? ................................ NN\n",
      "            's=True? .................................. NP$\n",
      "          as=True? .................................... CS\n",
      "        was=True? ..................................... BEDZ\n",
      "      is=True? ........................................ BEZ\n",
      "        his=False? .................................... BEZ\n",
      "        his=True? ..................................... PP$\n",
      "  ,=True? ............................................. ,\n",
      "the=True? ............................................. AT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree可以列出結構\n",
    "print classifier.pretty_format(depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting Context\n",
    "\n",
    "Context是指一個單字的前後文，例如單字fly可以作為名詞或動詞，如果前面是a或the，表示是名詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suf_1\": sentence[i][-1:], \"suf_2\": sentence[i][-2:], \"suf_3\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features['prev'] = '*'\n",
    "    else:\n",
    "        features['prev'] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev': u'an', 'suf_1': u'n', 'suf_2': u'on', 'suf_3': u'ion'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'an', u'investigation', u'of']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()[0][7:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   suf_1 = u'.'                . : NN     =   6950.8 : 1.0\n",
      "                   suf_2 = u'he'              AT : NN     =   3296.2 : 1.0\n",
      "                   suf_2 = u'ho'             WPS : NN     =   2982.4 : 1.0\n",
      "                   suf_1 = u'r'              JJR : NNS    =   2252.6 : 1.0\n",
      "                   suf_2 = u'to'              TO : JJ     =   2180.6 : 1.0\n",
      "                   suf_1 = u'h'              ABX : NNS    =   2013.7 : 1.0\n",
      "                   suf_2 = u'es'             NNS : IN     =   1676.3 : 1.0\n",
      "                   suf_3 = u'hat'             CS : NN     =   1576.4 : 1.0\n",
      "                   suf_1 = u\"'\"               '' : JJ     =   1502.2 : 1.0\n",
      "                   suf_2 = u'ng'             VBG : VBN    =   1241.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification\n",
    "\n",
    "可以更進一步，將單字預測的詞性，用來預測下一個單字的詞性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suf_1\": sentence[i][-1:], \"suf_2\": sentence[i][-2:], \"suf_3\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features['prev-word'] = '*'\n",
    "        features['prev-tag'] = '*'\n",
    "    else:\n",
    "        features['prev-word'] = sentence[i-1]\n",
    "        features['prev-tag'] = history[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自己定義自己的tagger class\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            # 利用enumerate產生 (index, value)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7981455725544738"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "產生test set的方法: 利用`random.shuffle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "random.shuffle(tagged_sents)\n",
    "# 用10%的資料作為test set\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy: 代表test set中，被正確分類的機率。\n",
    "* Precision: 代表被分類到X的資料中，真的是X的機率。\n",
    "* Recall: 代表所有X的資料中，能正確被分類到X的機率。\n",
    "* F-Measure(F-Score): 代表Precision及Recall的調和平均數=$\\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "以法庭為例，Precision是被判有罪的人裡面，真的有罪的機率。如果Precision是0.95，表示有5%的人會被誤判。Recall是所有罪犯會被判有罪的機率，如果Recall是0.85，表示有15%的人會被誤放。而F-Score則為(2\\*0.85\\*0.95)/(0.85+0.95)=0.897。\n",
    "\n",
    "### Confusion Matrices\n",
    "\n",
    "Confusion Matrix是用來分析輸入和輸出的分類。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "要利用機率模型來分類，最簡單的方法就是直接使用機率最高的分類，例如unigram model中，所有的字都預測為'the'。當我們已知機率模型$P(\\text{label | feature})$，對於未來新的輸入，就可以用$\\max_\\ell P(\\ell|\\text{feature})$來預測。\n",
    "\n",
    "$$\n",
    "P(\\text{label | feature}) =\n",
    "\\frac{P(\\text{feature, label})}{P(\\text{feature})} =\n",
    "\\frac{P(\\text{feature | label}) \\times P(\\text{label})}{P(\\text{feature})} =\n",
    "\\frac{ \\prod_{f \\in \\text{feature}} P(\\text{f | label}) \\times P(\\text{label})}{P(\\text{feature})}\n",
    "$$\n",
    "\n",
    "在實務上，我們會用$count(f,label) / count(label)$來估計$P(\\text{f | label})$的值。在POS問題中，假設我們想找P(suffix_es|NN)，則估計值為count(suffix_es|NN) / count(NN)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
