{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, json, os, nltk, string, gensim, bz2\n",
    "from gensim import corpora, models, similarities, utils\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import codecs\n",
    "import sys\n",
    "stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n",
    "reload(sys)\n",
    "sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "fmtstr = '%(asctime)s [%(levelname)s][%(name)s] %(message)s'\n",
    "datefmtstr = '%Y/%m/%d %H:%M:%S'\n",
    "log_fn = str(dt.now().date()) + '.txt'\n",
    "logger = logging.getLogger()\n",
    "if len(logger.handlers) >= 1:\n",
    "    logger.removeHandler(a.handlers[0])\n",
    "    logger.addHandler(logging.FileHandler(log_fn))\n",
    "    logger.handlers[0].setFormatter(logging.Formatter(fmtstr, datefmtstr))\n",
    "else:\n",
    "    logging.basicConfig(filename=log_fn, format=fmtstr,\n",
    "                        datefmt=datefmtstr, level=logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def docs_out(line):\n",
    "    j = json.loads(line)\n",
    "    tmp  = j.get('brief') + j.get('claim') + j.get('description')\n",
    "    tmp = re.sub('([,?!:;%$&*#~\\<\\>=+/\"(){}\\[\\]\\'])',' ',tmp)\n",
    "    tmp = tmp.replace(u\"\\u2018\", \" \").replace(u\"\\u2019\", \" \").replace(u\"\\u201c\",\" \").replace(u\"\\u201d\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2022\", \" \").replace(u\"\\u2013\", \" \").replace(u\"\\u2014\", \" \").replace(u\"\\u2026\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u20ac\", \" \").replace(u\"\\u201a\", \" \").replace(u\"\\u201e\", \" \").replace(u\"\\u2020\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2021\", \" \").replace(u\"\\u02C6\", \" \").replace(u\"\\u2030\", \" \").replace(u\"\\u2039\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u02dc\", \" \").replace(u\"\\u203a\", \" \").replace(u\"\\ufffe\", \" \").replace(u\"\\u00b0\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u00b1\", \" \").replace(u\"\\u0020\", \" \").replace(u\"\\u00a0\", \" \").replace(u\"\\u1680\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2000\", \" \").replace(u\"\\u2001\", \" \").replace(u\"\\u2002\", \" \").replace(u\"\\u2003\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2004\", \" \").replace(u\"\\u2005\", \" \").replace(u\"\\u2006\", \" \").replace(u\"\\u2007\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2008\", \" \").replace(u\"\\u2009\", \" \").replace(u\"\\u200a\", \" \").replace(u\"\\u202f\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u205f\", \" \").replace(u\"\\u3000\", \" \").replace(u\"\\u20ab\", \" \").replace(u\"\\u201b\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u201f\", \" \").replace(u\"\\u2e02\", \" \").replace(u\"\\u2e04\", \" \").replace(u\"\\u2e09\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2e0c\", \" \").replace(u\"\\u2e1c\", \" \").replace(u\"\\u2e20\", \" \").replace(u\"\\u00bb\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2e03\", \" \").replace(u\"\\u2e05\", \" \").replace(u\"\\u2e0a\", \" \").replace(u\"\\u2e0d\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2e1d\", \" \").replace(u\"\\u2e21\", \" \").replace(u\"\\u2032\", \" \").replace(u\"\\u2031\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2033\", \" \").replace(u\"\\u2034\", \" \").replace(u\"\\u2035\", \" \").replace(u\"\\u2036\", \" \")\n",
    "    tmp = tmp.replace(u\"\\u2037\", \" \").replace(u\"\\u2038\", \" \")\n",
    "    tmp = re.sub('[.] ',' ',tmp)\n",
    "    return tmp, j.get('patentNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "f = codecs.open('/share/USPatentData/tokenized_appDate_2013/2013USPTOPatents_by_skip_1.txt.tokenized','r', 'UTF-8')\n",
    "for line in f:\n",
    "    documents.append(''.join(docs_out(line)[0]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([doc.split() for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop_words\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq <= 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "#dictionary.save('USPTO_2013.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc.split()) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = model_tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LsiModel`的參數\n",
    "\n",
    "* `num_topics=200`: 設定SVD分解後要保留的維度\n",
    "* `id2word`: 提供corpus的字典，方便將id轉換為word\n",
    "* `chunksize=20000`: 在記憶體中一次處理的量，值越大則占用記憶體越多，處理速度也越快\n",
    "* `decay=1.0`: 因為資料會切成chunk來計算，所以會分成新舊資料，當新的chunk進來時，decay是舊chunk的加權，如果設小於1.0的值，則舊的資料會慢慢「遺忘」\n",
    "* `distributed=False`: 是否開啟分散式計算，每個core會分到一塊chunk\n",
    "* `onepass=True`: 設為False強制使用multi-pass stochastic algoritm\n",
    "* `power_iters=2`: 在multi-pass時設定power iteration，越大則accuracy越高，但時間越久\n",
    "\n",
    "令$X$代表corpus的TF-IDF矩陣，作完SVD分解後，會得到左矩陣`lsi.projection.u`及singular value `lsi.projection.s`。\n",
    "\n",
    "$X = USV^T$, where $U \\in \\mathbb{R}^{|V|\\times m}$, $S \\in \\mathbb{R}^{m\\times m}$, $V \\in \\mathbb{R}^{m\\times |D|}$\n",
    "\n",
    "`lsi[X]`等同於$U^{-1}X=VS$。所以要求$V$的值，可以用$S^{-1}U^{-1}X$，也就是`lsi[X]`除以$S$。\n",
    "\n",
    "因為`lsi[X]`本身沒有值，只是一個generator，要先透過`gensim.matutils.corpus2dense`轉換成numpy array，再除以`lsi.projection.s`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
    "corpus_lsi = model_lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 計算V的方法，可以作為document vector\n",
    "docvec_lsi = gensim.matutils.corpus2dense(corpus_lsi, len(model_lsi.projection.s)).T / model_lsi.projection.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word vector直接用U的column vector\n",
    "wordsim_lsi = similarities.MatrixSimilarity(model_lsi.projection.u, num_features=model_lsi.projection.u.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第二個版本，word vector用U*S\n",
    "wordsim_lsi2 = similarities.MatrixSimilarity(model_lsi.projection.u * model_lsi.projection.s,\n",
    "                                            num_features=model_lsi.projection.u.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lsi_query(query, use_ver2=False):\n",
    "    qvec = model_lsi[model_tfidf[dictionary.doc2bow(query.split())]]\n",
    "    if use_ver2:\n",
    "        s = wordsim_lsi2[qvec]\n",
    "    else:\n",
    "        s = wordsim_lsi[qvec]\n",
    "    return [dictionary[i] for i in s.argsort()[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'duct', u'energy', u'thermosetting', u'sheet', u'thermoforming', u'heater', u'mold', u'transducer', u'zone', u'uvc']\n"
     ]
    }
   ],
   "source": [
    "print lsi_query('energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'private_use_areas', u'energy', u'chamber', u'transducer', u'heater', u'thermoforming', u'sheet', u'mold', u'zone', u'uvc']\n"
     ]
    }
   ],
   "source": [
    "print lsi_query('energy', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec的參數\n",
    "\n",
    "* `sentences`: 用來訓練的list of list of words，但不是必要的，因為可以先建好model，再慢慢丟資料訓練\n",
    "* `size=100`: vector的維度\n",
    "* `alpha=0.025`: 初始的學習速度\n",
    "* `window=5`: context window的大小\n",
    "* `min_count=5`: 出現次數小於min_count的單字直接忽略\n",
    "* `max_vocab_size`: 限制vocabulary的大小，如果單字太多，就忽略最少見的單字，預設為無限制\n",
    "* `sample=0.001`: subsampling，隨機刪除機率小於0.001的單字，兼具擴大context windows與減少stopword的功能\n",
    "* `seed=1`: 隨機產生器的random seed\n",
    "* `workers=3`: 在多核心的系統上，要用幾個核心來train\n",
    "* `min_alpha=0.0001`: 學習速度最後收斂的最小值\n",
    "* `sg=0`: 0表示用CBOW，1表示用skip-gram\n",
    "* `hs=0`: 1表示用hierarchical soft-max，0表示用negative sampling\n",
    "* `negative=5`: 表示使用幾組negative sample來訓練\n",
    "* `cbow_mean=1`: 在使用CBOW的前提下，0表示使用sum作為hidden layer，1表示使用mean作為hidden layer\n",
    "* `hashfxn=<build-in hash function>`: 隨機初始化weights使用的hash function\n",
    "* `iter=5`: 整個corpus要訓練幾次\n",
    "* `trim_rule`: None表示小於min_count的單字會被忽略，也可以指定一個function(word, count, min_count)，這個function的傳回值有三種，`util.RULE_DISCARD`、`util.RULE_KEEP`、`util.RULE_DEFAULT`。這個參數會影響dictionary的生成\n",
    "* `sorted_vocab=1`: 1表示在指定word index前，先按照頻率將單字排序\n",
    "* `batch_words=10000`: 要傳給worker的單字長度\n",
    "\n",
    "#### 訓練方法\n",
    "\n",
    "先產生一個空的model  \n",
    "`model_w2v = models.Word2Vec(size=200, sg=1)`  \n",
    "傳入一個list of words更新vocabulary  \n",
    "`sent = [['first','sent'], ['second','sent']]`  \n",
    "`model_w2v.build_vocab(sent)`  \n",
    "傳入一個list of words更新model  \n",
    "`model_w2v.train(sent)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = [doc.split() for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_w2v = models.Word2Vec(size=200, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 5.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit model_w2v.build_vocab(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 2min 45s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit model_w2v.train(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'specialized', 0.44409653544425964),\n",
       " (u'adaboost', 0.42892950773239136),\n",
       " (u'extreme', 0.4117093086242676),\n",
       " (u'general-domain', 0.40930068492889404),\n",
       " (u'bayesian', 0.40740323066711426),\n",
       " (u'bischof', 0.40500378608703613),\n",
       " (u'pets', 0.404163658618927),\n",
       " (u'ensemble', 0.3979285955429077),\n",
       " (u'classifiers', 0.39685603976249695),\n",
       " (u'davis', 0.3963455259799957)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.most_similar_cosmul(['deep','learning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Doc2Vec Model\n",
    "\n",
    "Doc2Vec的參數\n",
    "\n",
    "* `documents=None`: 用來訓練的document，可以是list of `TaggedDocument`，或`TaggedDocument` generator\n",
    "* `size=300`: vector的維度\n",
    "* `alpha=0.025`: 初始的學習速度\n",
    "* `window=8`: context window的大小\n",
    "* `min_count=5`: 出現次數小於min_count的單字直接忽略\n",
    "* `max_vocab_size=None`: 限制vocabulary的大小，如果單字太多，就忽略最少見的單字，預設為無限制\n",
    "* `sample=0`: subsampling，隨機刪除機率小於sample的單字，兼具擴大context windows與減少stopword的功能\n",
    "* `seed=1`: 隨機產生器的random seed\n",
    "* `workers=1`: 在多核心的系統上，要用幾個核心來train\n",
    "* `min_alpha=0.0001`: 學習速度最後收斂的最小值\n",
    "* `hs=1`: 1表示用hierarchical soft-max，0表示用negative sampling\n",
    "* `negative=0`: 表示使用幾組negative sample來訓練\n",
    "* `dbow_words=0`: 1表示同時訓練出word-vector(用skip-gram)及doc-vector(用DBOW)，0表示只訓練doc-vector\n",
    "* `dm=1`: 1表示用distributed memory(PV-DM)來訓練，0表示用distributed bag-of-word(PV-DBOW)來訓練\n",
    "* `dm_concat=0`: 1表示不要sum/average而用concatenation of context vectors，0表示用sum/average。使用concatenation會產生較大的model，而且輸入的vector長度會變長\n",
    "* `dm_mean=0`: 在使用DBOW而且dm_concat=0的前提下，0表示使用sum作為hidden layer，1表示使用mean作為hidden layer\n",
    "* `dm_tag_count=1`: 當dm_concat=1時，預期每個document有幾個document tags\n",
    "* `trim_rule=None`: None表示小於min_count的單字會被忽略，也可以指定一個function(word, count, min_count)，這個function的傳回值有三種，util.RULE_DISCARD、util.RULE_KEEP、util.RULE_DEFAULT。這個參數會影響dictionary的生成\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatentDocGenerator(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def __iter__(self):\n",
    "        f = codecs.open(self.filename, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            text, appnum = docs_out(line)\n",
    "            yield TaggedDocument(text.split(), appnum.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 38.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "doc = PatentDocGenerator('/share/USPatentData/tokenized_appDate_2013/2013USPTOPatents_by_skip_1.txt.tokenized')\n",
    "%timeit model_d2v = Doc2Vec(doc, size=200, window=8, sample=1e-5, hs=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = PatentDocGenerator('/share/USPatentData/tokenized_appDate_2013/2013USPTOPatents_by_skip_1.txt.tokenized')\n",
    "model_d2v = Doc2Vec(doc, size=200, window=8, sample=1e-5, hs=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'US00D689829', 0.4423035681247711),\n",
       " (u'US00D690799', 0.43438467383384705),\n",
       " (u'20140182392', 0.42934495210647583),\n",
       " (u'20150022662', 0.4291260838508606),\n",
       " (u'20140125536', 0.4286767244338989),\n",
       " (u'20130270568', 0.4284389019012451),\n",
       " (u'US008650834', 0.428107351064682),\n",
       " (u'20130247928', 0.4270566999912262),\n",
       " (u'20140183325', 0.4267898201942444),\n",
       " (u'20140016249', 0.4265807569026947)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2v.docvecs.most_similar(['20140187118'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Doc2Vec(size=200, window=8, sample=1e-5, hs=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.build_vocab(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257832"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'US00D689829', 0.4423035681247711),\n",
       " (u'US00D690799', 0.43438467383384705),\n",
       " (u'20140182392', 0.42934495210647583),\n",
       " (u'20150022662', 0.4291260838508606),\n",
       " (u'20140125536', 0.4286767244338989),\n",
       " (u'20130270568', 0.4284389019012451),\n",
       " (u'US008650834', 0.428107351064682),\n",
       " (u'20130247928', 0.4270566999912262),\n",
       " (u'20140183325', 0.4267898201942444),\n",
       " (u'20140016249', 0.4265807569026947)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.docvecs.most_similar(['20140187118'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Doc2Vec Model from 2013 USPTO Patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatentDocGenerator(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def __iter__(self):\n",
    "        f = codecs.open(self.filename, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            text, appnum = docs_out(line)\n",
    "            yield TaggedDocument(text.split(), appnum.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2v = Doc2Vec(size=200, window=8, sample=1e-5, hs=0, negative=5)\n",
    "root = '/share/USPatentData/tokenized_appDate_2013/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fn in sorted(listdir(root)):\n",
    "    doc = PatentDocGenerator(os.path.join(root, fn))\n",
    "    start = dt.now()\n",
    "    model_d2v.build_vocab(doc)\n",
    "    model_d2v.train(doc)\n",
    "    logging.info('{} training time: {}'.format(fn, str(dt.now() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2v.save(\"doc2vec_uspto_2013.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
